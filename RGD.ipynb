{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg as sp_linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(n1, n2, p):\n",
    "    \"\"\"\n",
    "    Generate a mask with probability p for each entry\n",
    "    :param int n1: number of rows\n",
    "    :param int n2: number of columns\n",
    "    :param float p: probability of observing an entry\n",
    "    \"\"\"\n",
    "    omega = np.round(0.5 * (np.random.random((n1, n2)) + p))\n",
    "    return omega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ort(p, r):\n",
    "    E = np.random.randn(p, r)\n",
    "    E = 1./10 * np.linalg.qr(E)[0]\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start parametrs\n",
    "p = 50\n",
    "q = 90\n",
    "r = 5 \n",
    "prob = 2 * r**2 / (p * q)\n",
    "K = 10\n",
    "template = generate_mask(p, q, prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init X, Y, templates\n",
    "X_0 = np.random.rand(p, q)\n",
    "eps = np.random.normal(0, 1, (p, q))\n",
    "omega = generate_mask(p, q, prob)\n",
    "X = X_0 * omega \n",
    "Y = (X_0 + eps) * omega\n",
    "templates = [generate_mask(p, q, prob) for i in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGD(Y,r, K, templates, reg=0.05, mu_0=0.5, mu_e=0.5, mu_g=0.5):\n",
    "    p,q = Y.shape\n",
    "    \n",
    "    #init U and V\n",
    "    z = np.random.rand(K)\n",
    "    U = generate_ort(p, r)\n",
    "    V = generate_ort(q, r)\n",
    "    \n",
    "    values = np.random.rand(r) \n",
    "    L = np.diag(np.sort(values))\n",
    "\n",
    "    #init E_u and E_v\n",
    "    E_u = generate_ort(p, r)\n",
    "    E_v = generate_ort(q, r)\n",
    "\n",
    "    #init G\n",
    "    values = np.random.rand(r) \n",
    "    sort_values = np.sort(values)[::-1]\n",
    "    G = np.diag(sort_values)\n",
    "\n",
    "\n",
    "    iter_num = 0\n",
    "    max_iter = 30\n",
    "    I_r = np.eye(r)\n",
    "\n",
    "    while iter_num < max_iter:\n",
    "        iter_num += 1\n",
    "        nabla_z = np.zeros(K)\n",
    "        nabla_U = 0\n",
    "        nabla_V = 0\n",
    "        nabla_l = 0\n",
    "        for k in range(K):\n",
    "            image_vector_err = abs(z[k] - np.sum(templates[k] * (U @ L @ V.T)))\n",
    "            nabla_z[k] += abs(np.sum(Y * templates[k]) - z[k]) - image_vector_err\n",
    "            nabla_U += image_vector_err * templates[k] @ V @ L\n",
    "            nabla_V += image_vector_err * templates[k].T @ U @ L\n",
    "            nabla_l += image_vector_err * U.T @ templates[k] @ V\n",
    "\n",
    "        nabla_U += - mu_0 * U @ (U.T @ U - I_r) - mu_e * (U - E_u)\n",
    "        nabla_V += - mu_0 * V @ (V.T @ V - I_r) - mu_e * (V - E_v)\n",
    "        nabla_l += - mu_g * G @ L\n",
    "\n",
    "        z = z - reg * nabla_z\n",
    "        U = U - reg * nabla_U \n",
    "        V = V - reg * nabla_V\n",
    "        L = L - reg * nabla_l\n",
    "\n",
    "\n",
    "    return U @ L @ V.T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = RGD(Y, r, K, templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true error: 1.068032045596148, observed error: 0.12116415106913274\n"
     ]
    }
   ],
   "source": [
    "true_error = np.linalg.norm(X_new - X_0, ord='fro') / np.linalg.norm(X_0)\n",
    "observed_error = np.linalg.norm((X_new - X_0) * omega, ord='fro') / np.linalg.norm(X_0)\n",
    "print('true error: {}, observed error: {}'.format(true_error, observed_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_2 = RGD(Y, r, K, templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true error: 1.604797374338185e+27, observed error: 2.1756124773619673e+26\n"
     ]
    }
   ],
   "source": [
    "true_error = np.linalg.norm(X_new_2 - X_0, ord='fro') / np.linalg.norm(X_0)\n",
    "observed_error = np.linalg.norm((X_new_2 - X_0) * omega, ord='fro') / np.linalg.norm(X_0)\n",
    "print('true error: {}, observed error: {}'.format(true_error, observed_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Торчовская реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: -69.4010009765625\n",
      "Iteration 2, Loss: -100.46903991699219\n",
      "Iteration 3, Loss: -76.88442993164062\n",
      "Iteration 4, Loss: -24.386783599853516\n",
      "Iteration 5, Loss: -16.76999282836914\n",
      "Iteration 6, Loss: -15.50299072265625\n",
      "Iteration 7, Loss: -14.509227752685547\n",
      "Iteration 8, Loss: -13.681687355041504\n",
      "Iteration 9, Loss: -12.972997665405273\n",
      "Iteration 10, Loss: -12.355308532714844\n",
      "Iteration 11, Loss: -11.810039520263672\n",
      "Iteration 12, Loss: -11.323915481567383\n",
      "Iteration 13, Loss: -10.887015342712402\n",
      "Iteration 14, Loss: -10.491703033447266\n",
      "Iteration 15, Loss: -10.131949424743652\n",
      "Iteration 16, Loss: -9.802913665771484\n",
      "Iteration 17, Loss: -9.500640869140625\n",
      "Iteration 18, Loss: -9.221863746643066\n",
      "Iteration 19, Loss: -8.963849067687988\n",
      "Iteration 20, Loss: -8.724296569824219\n",
      "Iteration 21, Loss: -8.501241683959961\n",
      "Iteration 22, Loss: -8.29300594329834\n",
      "Iteration 23, Loss: -8.098136901855469\n",
      "Iteration 24, Loss: -7.915374755859375\n",
      "Iteration 25, Loss: -7.743617057800293\n",
      "Iteration 26, Loss: -7.581897735595703\n",
      "Iteration 27, Loss: -7.429361343383789\n",
      "Iteration 28, Loss: -7.285251617431641\n",
      "Iteration 29, Loss: -7.1488938331604\n",
      "Iteration 30, Loss: -7.019683837890625\n",
      "Iteration 31, Loss: -6.8970842361450195\n",
      "Iteration 32, Loss: -6.7806077003479\n",
      "Iteration 33, Loss: -6.669812202453613\n",
      "Iteration 34, Loss: -6.564301490783691\n",
      "Iteration 35, Loss: -6.463716506958008\n",
      "Iteration 36, Loss: -6.36772346496582\n",
      "Iteration 37, Loss: -6.27602481842041\n",
      "Iteration 38, Loss: -6.1883440017700195\n",
      "Iteration 39, Loss: -6.104429244995117\n",
      "Iteration 40, Loss: -6.024050712585449\n",
      "Iteration 41, Loss: -5.946991920471191\n",
      "Iteration 42, Loss: -5.873058795928955\n",
      "Iteration 43, Loss: -5.802069187164307\n",
      "Iteration 44, Loss: -5.733855247497559\n",
      "Iteration 45, Loss: -5.66826057434082\n",
      "Iteration 46, Loss: -5.605137348175049\n",
      "Iteration 47, Loss: -5.544355869293213\n",
      "Iteration 48, Loss: -5.485787391662598\n",
      "Iteration 49, Loss: -5.429317951202393\n",
      "Iteration 50, Loss: -5.374835968017578\n",
      "Iteration 51, Loss: -5.322239398956299\n",
      "Iteration 52, Loss: -5.271435737609863\n",
      "Iteration 53, Loss: -5.222332000732422\n",
      "Iteration 54, Loss: -5.174846172332764\n",
      "Iteration 55, Loss: -5.128899574279785\n",
      "Iteration 56, Loss: -5.084417343139648\n",
      "Iteration 57, Loss: -5.0413312911987305\n",
      "Iteration 58, Loss: -4.999574661254883\n",
      "Iteration 59, Loss: -4.959085464477539\n",
      "Iteration 60, Loss: -4.919806480407715\n",
      "Iteration 61, Loss: -4.881681442260742\n",
      "Iteration 62, Loss: -4.844657897949219\n",
      "Iteration 63, Loss: -4.808688640594482\n",
      "Iteration 64, Loss: -4.773725509643555\n",
      "Iteration 65, Loss: -4.739725112915039\n",
      "Iteration 66, Loss: -4.706644535064697\n",
      "Iteration 67, Loss: -4.674444198608398\n",
      "Iteration 68, Loss: -4.643086910247803\n",
      "Iteration 69, Loss: -4.612536430358887\n",
      "Iteration 70, Loss: -4.582757949829102\n",
      "Iteration 71, Loss: -4.553720951080322\n",
      "Iteration 72, Loss: -4.525392532348633\n",
      "Iteration 73, Loss: -4.497743606567383\n",
      "Iteration 74, Loss: -4.470746994018555\n",
      "Iteration 75, Loss: -4.4443745613098145\n",
      "Iteration 76, Loss: -4.418601036071777\n",
      "Iteration 77, Loss: -4.393402576446533\n",
      "Iteration 78, Loss: -4.36875581741333\n",
      "Iteration 79, Loss: -4.344636917114258\n",
      "Iteration 80, Loss: -4.321025848388672\n",
      "Iteration 81, Loss: -4.297901630401611\n",
      "Iteration 82, Loss: -4.275245666503906\n",
      "Iteration 83, Loss: -4.253036975860596\n",
      "Iteration 84, Loss: -4.231260299682617\n",
      "Iteration 85, Loss: -4.209896087646484\n",
      "Iteration 86, Loss: -4.188928604125977\n",
      "Iteration 87, Loss: -4.168341636657715\n",
      "Iteration 88, Loss: -4.148121356964111\n",
      "Iteration 89, Loss: -4.128252029418945\n",
      "Iteration 90, Loss: -4.1087188720703125\n",
      "Iteration 91, Loss: -4.0895094871521\n",
      "Iteration 92, Loss: -4.070611000061035\n",
      "Iteration 93, Loss: -4.052011013031006\n",
      "Iteration 94, Loss: -4.033697128295898\n",
      "Iteration 95, Loss: -4.015657901763916\n",
      "Iteration 96, Loss: -3.9978833198547363\n",
      "Iteration 97, Loss: -3.9803614616394043\n",
      "Iteration 98, Loss: -3.963083267211914\n",
      "Iteration 99, Loss: -3.9460387229919434\n",
      "Iteration 100, Loss: -3.929218292236328\n"
     ]
    }
   ],
   "source": [
    "def loss(U, V, Lambda, z, Z, Tau, g, mu_g, mu_o, mu_e, E, E_prime):\n",
    "    K = Tau.shape[0]\n",
    "    r = Lambda.shape[0]\n",
    "    loss1 = -0.5 * torch.norm(Z - z)**2\n",
    "    loss2 = 0.0\n",
    "    for k in range(K):\n",
    "        pred = torch.trace(Tau[k] @ (U @ Lambda @ V.T))\n",
    "        loss2 -= 0.5 * (z[k] - pred)**2\n",
    "    loss3 = -0.5 * mu_g * torch.sum(g**2 * torch.diag(Lambda)**2)\n",
    "    loss4 = -0.5 * mu_o * (torch.norm(U.T @ U - torch.eye(r), 'fro')**2 + torch.norm(V.T @ V - torch.eye(r), 'fro')**2)\n",
    "    loss5 = -0.5 * mu_e * (torch.norm(U - E, 'fro')**2 + torch.norm(V - E_prime, 'fro')**2)\n",
    "    \n",
    "    return loss1 + loss2 + loss3 + loss4 + loss5\n",
    "  \n",
    "def gradient_descent(U, V, Lambda_diag, z, Z, Tau, g, mu_g, mu_o, mu_e, E, E_prime, lr=0.01, max_iter=100):\n",
    "    optimizer = torch.optim.SGD([U, V, Lambda_diag, z], lr=lr, maximize=True)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "        Lambda = torch.diag(Lambda_diag)\n",
    "        current_loss = loss(U, V, Lambda, z, Z, Tau, g, mu_g, mu_o, mu_e, E, E_prime)\n",
    "        # print(U)\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Iteration {i + 1}, Loss: {current_loss.item()}\")\n",
    "    \n",
    "    return U, V, Lambda_diag, z\n",
    "\n",
    "\n",
    "p, q, r, K = 10, 10, 3, 4 \n",
    "#тут нужно U и V сделать ортогональными \n",
    "U = torch.randn(p, r, requires_grad=True)\n",
    "V = torch.randn(q, r, requires_grad=True)\n",
    "Lambda_diag = torch.linspace(start=1.0, end=0.1, steps=r).requires_grad_()\n",
    "z = torch.randn(K, requires_grad=True)\n",
    "Z = torch.randn(K)\n",
    "Tau = torch.randint(0, 2, (K, p, q), dtype=torch.float32)\n",
    "g = torch.linspace(start=0.1, end=1.0, steps=r)\n",
    "mu_g, mu_o, mu_e = 0.1, 0.1, 0.1\n",
    "E = torch.randn(p, r)\n",
    "E_prime = torch.randn(q, r)\n",
    "\n",
    "U_opt, V_opt, Lambda_diag_opt, z_opt = gradient_descent(U, V, Lambda_diag, z, Z, Tau, g, mu_g, mu_o, mu_e, E, E_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
